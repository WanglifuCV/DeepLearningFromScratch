{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1:\n",
    "            t = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        \n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "            \n",
    "        # set\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 梯度确认"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.mnist import load_mnist\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48550  1559  2627  7831  3742 14589  3130 39018  4416 42500 41053 13379\n",
      " 27479 36670  6505 44528 24121  9818  3254  2269 40879 29709 39629 24468\n",
      " 28501 45938 41638  2909 33246 40749 43736 37237 10336  7664 43117 44761\n",
      " 49548 34426 41596 31501 42796 16867 34454 25814   814 31485 28362 47017\n",
      " 23105 10979  9700 40343 10147 45716 41095 43231 39060 42690 33673  6636\n",
      " 39915 24206 22279 31214 41876 10652 45907  8992 47157 44460 32565 25877\n",
      " 39505 21124 45081 24845 14457  5198  2434 31109 13405 37339 22217 35432\n",
      " 49643 44285 31154  4410  4624 28072 24364 42730 28115 40788  9919 11892\n",
      "  2457 37376  6745 43163]\n",
      "numerical_time : 27.76185441017151\n",
      "BP_time : 0.0\n",
      "W1:4.260218857633593e-07\n",
      "b1:3.5495850382466573e-06\n",
      "W2:1.8631792694221478e-09\n",
      "b2:2.195566202833725e-08\n"
     ]
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "indexes = np.random.choice(50000, 100)\n",
    "print(indexes)\n",
    "x_batch = x_train[indexes, :]\n",
    "t_batch = t_train[indexes, :]\n",
    "\n",
    "time1 = time.time()\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "time2 = time.time()\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "time3 = time.time()\n",
    "numerical_time = time2  -time1\n",
    "bp_time = time3  -time3\n",
    "\n",
    "\n",
    "print(\"numerical_time : \" + str(numerical_time))\n",
    "print(\"BP_time : \" + str(bp_time))\n",
    "# Mean error\n",
    "\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\n",
    "    print(key + ':' + str(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007309933796620397 0.99865 0.9742\n",
      "0.005026910107174594 0.9988833333333333 0.9745\n",
      "0.0064488773844279335 0.9988333333333334 0.9751\n",
      "0.007542571474249563 0.9988833333333333 0.9749\n",
      "0.006167556815645264 0.9989166666666667 0.9748\n",
      "0.0070022759872610295 0.9986333333333334 0.9752\n",
      "0.007571945699918553 0.9990333333333333 0.9751\n",
      "0.004933892746092766 0.9986833333333334 0.974\n",
      "0.0023132983469707825 0.9988833333333333 0.9748\n",
      "0.00293023534058304 0.99895 0.974\n",
      "0.007708118955182428 0.9990833333333333 0.9742\n",
      "0.01478168765111227 0.9989333333333333 0.9744\n",
      "0.010026757899861427 0.9990833333333333 0.9744\n",
      "0.007554113899386757 0.999 0.9749\n",
      "0.01024009711515909 0.9991166666666667 0.9749\n",
      "0.0023094473887613366 0.99855 0.9746\n",
      "0.007085184946794808 0.9988166666666667 0.9744\n",
      "0.005082751855548628 0.9989666666666667 0.9747\n",
      "0.011319490732121504 0.9992166666666666 0.9753\n",
      "0.004172708723333639 0.9987 0.9738\n",
      "0.008020747836345331 0.9987 0.9748\n",
      "0.0021294373502444624 0.9988833333333333 0.9746\n",
      "0.004413340644653655 0.9990666666666667 0.9749\n",
      "0.0024444336620123657 0.9984666666666666 0.974\n",
      "0.006056663912932355 0.99905 0.9748\n",
      "0.005680695005340626 0.9986833333333334 0.9739\n",
      "0.0039003850773388325 0.999 0.9744\n",
      "0.003960709280792874 0.9991333333333333 0.9745\n",
      "0.003746190922941023 0.9992166666666666 0.9744\n",
      "0.007422459438639017 0.9992333333333333 0.9748\n",
      "0.004331863144573752 0.999 0.9749\n",
      "0.00497285025955252 0.9991833333333333 0.9745\n",
      "0.003700124708714886 0.9992166666666666 0.9745\n",
      "0.005833253292849354 0.9990833333333333 0.9745\n",
      "0.009228516007280092 0.99915 0.975\n",
      "0.0024792494565424586 0.9992 0.9751\n",
      "0.005659879373965013 0.9989833333333333 0.9746\n",
      "0.0019767520960667674 0.9993666666666666 0.975\n",
      "0.0038435658446574063 0.9993333333333333 0.9753\n",
      "0.004200849582182729 0.9993166666666666 0.9746\n",
      "0.0034485610391561507 0.9994166666666666 0.9753\n",
      "0.0007019625615644009 0.9988 0.9746\n",
      "0.0014690144724210288 0.9993833333333333 0.9744\n",
      "0.0057185887733223065 0.9993166666666666 0.9734\n",
      "0.005398501473556913 0.9993 0.975\n",
      "0.004070501472276753 0.99915 0.9747\n",
      "0.0061753289258512 0.99935 0.9751\n",
      "0.005151585050158331 0.99945 0.9752\n",
      "0.00704815317222569 0.99865 0.9745\n",
      "0.0028286951497071985 0.9994333333333333 0.9748\n",
      "0.002606576355766812 0.99945 0.9748\n",
      "0.002733458264254936 0.9993333333333333 0.9747\n",
      "0.0044142856345196595 0.9993333333333333 0.9736\n",
      "0.005566709759113536 0.9992833333333333 0.9739\n",
      "0.0030630240612698513 0.9995166666666667 0.9748\n",
      "0.010343840038022023 0.99945 0.974\n",
      "0.008117141384073075 0.9994166666666666 0.9748\n",
      "0.00832291089156551 0.9994666666666666 0.9747\n",
      "0.006590250996525516 0.9995833333333334 0.9746\n",
      "0.003157965616786892 0.9996 0.9745\n",
      "0.003349239849268075 0.9994333333333333 0.974\n",
      "0.002114296786790615 0.9994833333333333 0.975\n",
      "0.006129850811079335 0.99945 0.9745\n",
      "0.005696730202246313 0.9995 0.9748\n",
      "0.0027106538497642064 0.9991666666666666 0.9747\n",
      "0.00575984129397022 0.9994166666666666 0.9744\n",
      "0.005251062394831075 0.9994166666666666 0.9745\n"
     ]
    }
   ],
   "source": [
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # gradient:\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # update:\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % (iter_per_epoch / 4) == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(loss, train_acc, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
